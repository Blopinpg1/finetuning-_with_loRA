{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blopinpg1/finetuning-_with_loRA/blob/main/finetuning__with_loRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcyGuMi9M-LT"
      },
      "source": [
        "#SETUP AND CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve_yzMrVwLwH"
      },
      "outputs": [],
      "source": [
        "# @title Install the Dependencies and Set Everything Up {\"display-mode\": \"form\"}\n",
        "\n",
        "!pip install transformers datasets accelerate bitsandbytes -q\n",
        "\n",
        "# - transformers: For models and tokenizers\n",
        "# - datasets: To easily load and process our training data\n",
        "# - accelerate: A library from Hugging Face to simplify training on any infrastructure (like the Colab GPU)\n",
        "# - bitsandbytes: For quantization to make training more memory-efficient\n",
        "\n",
        "# Import the required libraries\n",
        "import pprint\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from google.colab import output\n",
        "import pprint\n",
        "import peft\n",
        "\n",
        "# Suppress verbose output from transformers\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "output.clear()\n",
        "\n",
        "print(\"ü§ò The setup is complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T3Z9jXKM6st"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLuaTAN4NV16"
      },
      "source": [
        "#FINDING AND PREPARING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTNILfkGEESH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Someman/news_nepali\", split=\"train\")\n",
        "dataset\n",
        "\n",
        "dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZV1CazmKjgs"
      },
      "outputs": [],
      "source": [
        "def format_prompt(example):\n",
        "    instruction = \"‡§Ø‡•Ä ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞‡§ï‡•ã ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞ ‡§≤‡•á‡§ñ‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç:\"\n",
        "\n",
        "    article = example[\"article\"]\n",
        "    summary = example[\"article_summary\"]\n",
        "\n",
        "    prompt = (\n",
        "        f\"<s>[INST] {instruction}\\n\\n\"\n",
        "        f\"{article} [/INST] \"\n",
        "        f\"{summary} </s>\"\n",
        "    )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "\n",
        "formatted_dataset = dataset.map(format_prompt)\n",
        "\n",
        "# This is what the first examplwe will look like.\n",
        "print(formatted_dataset[0]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO2NrzWVNgWE"
      },
      "source": [
        "# Loading the Pre-Trained Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8-B8VMyDH_K"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Quantization configuration to load the model in 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "def get_model():\n",
        "    # Load the model with our quantization configuration\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    output.clear()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Disable cache to prepare for training\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence token.\n",
        "# This is a common practice for decoder-only models.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"[EOS]\", tokenizer.pad_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vIjXceVGAER"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = peft.LoraConfig(\n",
        "    r=8,  # Rank of the update matrices.\n",
        "    lora_alpha=32,  # Scaling factor for the LoRA weights.\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers.\n",
        "    bias=\"none\",  # Bias type (none, all, or lora_only).\n",
        "    task_type=\"CAUSAL_LM\",  # Task type (e.g., CAUSAL_LM for language generation).\n",
        "    fan_in_fan_out=True, # Explicitly set for Conv1D layers\n",
        ")\n",
        "\n",
        "# Add LoRA adapters to the model\n",
        "model = peft.get_peft_model(get_model(), lora_config)\n",
        "\n",
        "# Print the trainable parameters to see the effect of LoRA\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XybMtbYBbApY"
      },
      "source": [
        "#tokenize the dataset of our new formatted strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izYsl2jvYhyI"
      },
      "outputs": [],
      "source": [
        "def tokenize_nepali_dataset(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes the formatted instruction-response text for LLaMA/LoRA training.\n",
        "\n",
        "    Args:\n",
        "        examples: a batch of examples from formatted_dataset, each with a 'text' key\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with 'input_ids' and 'attention_mask' for the model\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],        # the formatted <instruction>‚Ä¶<response> text\n",
        "        padding=\"max_length\",    # pad all sequences to max_length\n",
        "        truncation=True,         # truncate sequences longer than max_length\n",
        "        max_length=256          # adjust this to fit your GPU/memory\n",
        "    )\n",
        "\n",
        "# Tokenize the entire dataset\n",
        "tokenized_dataset = formatted_dataset.map(tokenize_nepali_dataset, batched=True)\n",
        "small_dataset = tokenized_dataset.select(range(1000))\n",
        "\n",
        "\n",
        "# Optional: inspect the first example\n",
        "import pprint\n",
        "pprint.pp(tokenized_dataset[0], compact=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING ARGUMENTS"
      ],
      "metadata": {
        "id": "HlXVjS6Yj9Ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hLAvCjdbpyE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nepali-lora-summarization\",  # Save directory for your LoRA model\n",
        "    num_train_epochs=1,                        # Train for 3 epochs (can adjust)\n",
        "    per_device_train_batch_size=1,             # 1 example per GPU (safe for 7B in 4-bit)\n",
        "    gradient_accumulation_steps=4,             # Simulates larger batch size\n",
        "    learning_rate=2e-4,                        # LoRA-friendly learning rate\n",
        "    fp16=True,                                 # Mixed precision for faster training\n",
        "    logging_steps=50,                          # Log every 50 steps\n",
        "    save_steps=500,                            # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,                        # Keep only last 2 checkpoints\n",
        "    report_to=\"none\",                          # Disable logging to W&B or other services\n",
        "    remove_unused_columns=True,\n",
        "    # lr_scheduler_type=\"cosine\",\n",
        "    # warmup_steps=50,\n",
        "\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                     # LoRA-wrapped model\n",
        "    args=training_args,               # Training arguments from above\n",
        "    train_dataset=small_dataset,      # small part  tokenized Nepali dataset\n",
        "    tokenizer=tokenizer,              # Needed for saving the model correctly\n",
        "    data_collator=data_collator       # Prepares batches\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FINE TUNING"
      ],
      "metadata": {
        "id": "jn6Dor7Yjx53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3PJuUOmdUNA"
      },
      "outputs": [],
      "source": [
        "# Let's start fine-tuning!\n",
        "print(\"üöÄ Starting fine-tuning‚Ä¶\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Fine-tuning complete!\")\n",
        "\n",
        "# This saves the final model and tokenizer to the output directory\n",
        "final_model_dir = \"./nepali-lora-summarization-final\"\n",
        "trainer.save_model(final_model_dir)\n",
        "print(f\"Model saved to {final_model_dir}\")\n",
        "\n",
        "# Clear model and trainer to free up GPU memory before inference\n",
        "del model\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU memory cleared after training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING OUR MODEL"
      ],
      "metadata": {
        "id": "0yemTxrgjrkM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WGgp1t4eQRh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc # Import gc for garbage collection\n",
        "\n",
        "# Original article to be summarized\n",
        "original_article = \"‡§∏‡§§‡•ç‡§§‡§æ‡§∞‡•Å‡§¢ ‡§®‡•á‡§ï‡§™‡§æ (‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡•Ä ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞)‡§≤‡•á ‡§ö‡•à‡§§ ‡•ß‡•ß ‡§ó‡§§‡•á (‡§∏‡•ã‡§Æ‡§¨‡§æ‡§∞)‡§≠‡§ø‡§§‡•ç‡§∞ ‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§ø‡§™‡§∞‡§ø‡§∑‡§¶‡•ç‡§≤‡§æ‡§à ‡§™‡•Ç‡§∞‡•ç‡§£‡§§‡§æ ‡§¶‡§ø‡§®‡•á ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ó‡§∞‡•á‡§ï‡•ã ‡§õ‡•§ ‡§∂‡§®‡§ø‡§¨‡§æ‡§∞ ‡§¶‡§ø‡§â‡§Å‡§∏‡•ã ‡§™‡•á‡§∞‡§ø‡§∏‡§°‡§æ‡§Å‡§°‡§æ‡§Æ‡§æ ‡§¨‡§∏‡•á‡§ï‡•ã ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§™‡§¶‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä ‡§¨‡•à‡§†‡§ï‡§≤‡•á ‡§â‡§ï‡•ç‡§§ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ó‡§∞‡•á‡§ï‡•ã ‡§π‡•ã‡•§ ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡•Ä ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§ï‡•ã ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡•Ä‡§Ø ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø‡§ï‡§æ ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä‡§≤‡•á ‡§∏‡•ã‡§Æ‡§¨‡§æ‡§∞‡§∏‡§Æ‡•ç‡§Æ ‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§ø‡§™‡§∞‡§ø‡§∑‡§¶‡•ç ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§ó‡§∞‡•ç‡§®‡•á ‡§î‡§™‡§ö‡§æ‡§∞‡§ø‡§ï ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ó‡§∞‡•á‡§ï‡•ã ‡§∏‡§ö‡§ø‡§µ ‡§¶‡•á‡§µ‡•á‡§®‡•ç‡§¶‡•ç‡§∞ ‡§™‡•å‡§°‡•á‡§≤‡§≤‡•á ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡§ø‡§è‡•§ ‡§∏‡§ö‡§ø‡§µ ‡§™‡•å‡§°‡•á‡§≤‡§≤‡•á ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡§æ‡§à ‡§∏‡§ô‡•ç‡§ò‡•Ä‡§Ø ‡§∏‡§∞‡§ï‡§æ‡§∞‡§ï‡•ã ‡§®‡•Ä‡§§‡§ø ‡§§‡§•‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ‡§∏‡§Å‡§ó ‡§ú‡•ã‡§°‡•á‡§∞ ‡§ó‡§§‡§ø‡§∂‡§ø‡§≤ ‡§¨‡§®‡§æ‡§â‡§®‡•Å‡§™‡§∞‡•ç‡§®‡•á‡§Æ‡§æ ‡§®‡•á‡§§‡§æ‡§π‡§∞‡•Å‡§≤‡•á ‡§ú‡•ã‡§° ‡§¶‡§ø‡§è‡§ï‡•ã ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡§ø‡§Å‡§¶‡•à ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§®‡•á‡§§‡§æ‡§π‡§∞‡•Å‡§≤‡•á ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä ‡§™‡•Å‡§∑‡•ç‡§™‡§ï‡§Æ‡§≤ ‡§¶‡§æ‡§π‡§æ‡§≤‡§ï‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§®‡§æ‡§ï‡§∞‡•ç‡§∑‡§£ ‡§ó‡§∞‡§æ‡§è‡§ï‡§æ ‡§õ‡§®‡•ç‡•§ ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§∞ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§¨‡•Ä‡§ö‡§ï‡•ã ‡§ñ‡§æ‡§°‡§≤ ‡§ò‡§ü‡§æ‡§â‡§® ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑ ‡§°‡§æ.‡§≤‡•á ‡§™‡§®‡§ø ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•á‡§ï‡§æ ‡§õ‡§®‡•ç‡•§\"\n",
        "\n",
        "# Define the instruction for summarization as used in training\n",
        "instruction = \"‡§Ø‡•Ä ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞‡§ï‡•ã ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞ ‡§≤‡•á‡§ñ‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç:\"\n",
        "\n",
        "# Format the prompt for inference according to the training format\n",
        "formatted_prompt = (\n",
        "    f\"<s>[INST] {instruction}\\n\\n\"\n",
        "    f\"{original_article} [/INST] \"\n",
        ")\n",
        "\n",
        "print(\"--- Testing the Original Base Model ---\")\n",
        "\n",
        "# Load base model in 4-bit for inference\n",
        "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "base_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=base_model_for_inference, # Pass the loaded 4-bit model object\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "base_out = base_generator(\n",
        "    formatted_prompt, # Use the formatted prompt here\n",
        "    max_new_tokens=150,\n",
        "    do_sample=False,      # Better for summarization\n",
        ")\n",
        "\n",
        "print(\"Base model response:\")\n",
        "# Extract only the generated summary part by removing the input prompt\n",
        "base_generated_text = base_out[0][\"generated_text\"].replace(formatted_prompt, \"\").strip()\n",
        "# Remove the </s> token if present\n",
        "base_generated_text = base_generated_text.replace(\"</s>\", \"\").strip()\n",
        "print(base_generated_text)\n",
        "\n",
        "# Clear base model from memory before loading fine-tuned model\n",
        "del base_generator\n",
        "del base_model_for_inference\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Base model cleared from GPU memory.\")\n",
        "\n",
        "print(\"\\n--- Testing Our Fine-Tuned Nepali Model ---\")\n",
        "\n",
        "# Load base model with the same quantization config used during training\n",
        "base_model_for_finetune = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model_for_finetune, final_model_dir)\n",
        "\n",
        "# Merge LoRA into base model for inference\n",
        "fine_tuned_model = fine_tuned_model.merge_and_unload()\n",
        "\n",
        "# Create inference pipeline\n",
        "fine_tuned_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=fine_tuned_model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "ft_out = fine_tuned_generator(\n",
        "    formatted_prompt, # Use the formatted prompt here\n",
        "    max_new_tokens=250,\n",
        "    do_sample=False,      # <- IMPORTANT\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned model response:\")\n",
        "# Extract only the generated summary part by removing the input prompt\n",
        "ft_generated_text = ft_out[0][\"generated_text\"].replace(formatted_prompt, \"\").strip()\n",
        "# Remove the </s> token if present\n",
        "ft_generated_text = ft_generated_text.replace(\"</s>\", \"\").strip()\n",
        "print(ft_generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vj7G48jtVDsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPgwC9BXTQ0cE8j76WFFDLq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}